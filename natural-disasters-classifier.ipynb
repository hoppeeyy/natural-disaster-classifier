{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2434228,"sourceType":"datasetVersion","datasetId":1472998}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef\nimport numpy as np\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image, UnidentifiedImageError\n\nclass SafeImageFolder(ImageFolder): # to skip the broken or empty img file\n    def __getitem__(self, index):\n        while True:\n            try:\n                return super(SafeImageFolder, self).__getitem__(index)\n            except (UnidentifiedImageError, OSError) as e:\n                print(f\"Skipping broken image at index {index}: {e}\")\n                index = (index + 1) % len(self.imgs)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:50:38.936186Z","iopub.execute_input":"2025-04-28T23:50:38.936534Z","iopub.status.idle":"2025-04-28T23:50:46.429471Z","shell.execute_reply.started":"2025-04-28T23:50:38.936508Z","shell.execute_reply":"2025-04-28T23:50:46.428804Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Check if GPU is working or CPU","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:51:24.087353Z","iopub.execute_input":"2025-04-28T23:51:24.088158Z","iopub.status.idle":"2025-04-28T23:51:24.164553Z","shell.execute_reply.started":"2025-04-28T23:51:24.088129Z","shell.execute_reply":"2025-04-28T23:51:24.163524Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Transformation and Resize","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:51:27.361722Z","iopub.execute_input":"2025-04-28T23:51:27.362434Z","iopub.status.idle":"2025-04-28T23:51:27.366093Z","shell.execute_reply.started":"2025-04-28T23:51:27.362406Z","shell.execute_reply":"2025-04-28T23:51:27.365548Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Loading dataset","metadata":{}},{"cell_type":"code","source":"full_dataset = SafeImageFolder(root='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)', transform=transform)\n\n\ntrain_size = int(0.8 * len(full_dataset)) # 80% into training \nval_size = len(full_dataset) - train_size # 20% nto validation\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:51:31.902738Z","iopub.execute_input":"2025-04-28T23:51:31.903032Z","iopub.status.idle":"2025-04-28T23:51:54.576260Z","shell.execute_reply.started":"2025-04-28T23:51:31.903009Z","shell.execute_reply":"2025-04-28T23:51:54.575703Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Defining model","metadata":{}},{"cell_type":"code","source":"model = models.densenet121(weights=True) # weights=true means weights are pre-trained\n\n\nnum_features = model.classifier.in_features\nnum_classes = len(full_dataset.classes)\n\nmodel.classifier = nn.Linear(num_features, num_classes)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:52:01.852148Z","iopub.execute_input":"2025-04-28T23:52:01.852770Z","iopub.status.idle":"2025-04-28T23:52:02.684013Z","shell.execute_reply.started":"2025-04-28T23:52:01.852745Z","shell.execute_reply":"2025-04-28T23:52:02.683332Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 221MB/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Defining loss function","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:52:09.594948Z","iopub.execute_input":"2025-04-28T23:52:09.595586Z","iopub.status.idle":"2025-04-28T23:52:09.600853Z","shell.execute_reply.started":"2025-04-28T23:52:09.595563Z","shell.execute_reply":"2025-04-28T23:52:09.600046Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"epochs = 15\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:52:14.400513Z","iopub.execute_input":"2025-04-28T23:52:14.400867Z","iopub.status.idle":"2025-04-29T00:23:08.065184Z","shell.execute_reply.started":"2025-04-28T23:52:14.400841Z","shell.execute_reply":"2025-04-29T00:23:08.064403Z"}},"outputs":[{"name":"stdout","text":"Skipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 1/15, Loss: 0.4101\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 2/15, Loss: 0.2853\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 3/15, Loss: 0.2434\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 4/15, Loss: 0.2116\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 5/15, Loss: 0.1949\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 6/15, Loss: 0.1632\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 7/15, Loss: 0.1662\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 8/15, Loss: 0.1650\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 9/15, Loss: 0.1305\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 10/15, Loss: 0.1288\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 11/15, Loss: 0.1086\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 12/15, Loss: 0.0991\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 13/15, Loss: 0.0839\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 14/15, Loss: 0.1022\nSkipping broken image at index 2455: cannot identify image file <_io.BufferedReader name='/kaggle/input/disaster-images-dataset/Comprehensive Disaster Dataset(CDD)/Human_Damage/02_0069.png'>\nEpoch 15/15, Loss: 0.0815\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Evulation","metadata":{}},{"cell_type":"code","source":"model.eval()\nall_preds = []\nall_labels = []\nall_probs = []\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n        _, preds = torch.max(outputs, 1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n# Convert to numpy arrays\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\nall_probs = np.array(all_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:23:46.322435Z","iopub.execute_input":"2025-04-29T00:23:46.322718Z","iopub.status.idle":"2025-04-29T00:24:22.748716Z","shell.execute_reply.started":"2025-04-29T00:23:46.322697Z","shell.execute_reply":"2025-04-29T00:24:22.747937Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Calculate Metrices","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\nmcc = matthews_corrcoef(all_labels, all_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:24:25.285321Z","iopub.execute_input":"2025-04-29T00:24:25.285588Z","iopub.status.idle":"2025-04-29T00:24:25.305603Z","shell.execute_reply.started":"2025-04-29T00:24:25.285569Z","shell.execute_reply":"2025-04-29T00:24:25.305014Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### AUC","metadata":{}},{"cell_type":"code","source":"try:\n    auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\", average=\"weighted\")\nexcept:\n    auc = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:24:28.496923Z","iopub.execute_input":"2025-04-29T00:24:28.497205Z","iopub.status.idle":"2025-04-29T00:24:28.511590Z","shell.execute_reply.started":"2025-04-29T00:24:28.497176Z","shell.execute_reply":"2025-04-29T00:24:28.510941Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"AUC: {auc if auc is not None else 'Not available'}\")\nprint(f\"MCC: {mcc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:24:31.380159Z","iopub.execute_input":"2025-04-29T00:24:31.380488Z","iopub.status.idle":"2025-04-29T00:24:31.384985Z","shell.execute_reply.started":"2025-04-29T00:24:31.380467Z","shell.execute_reply":"2025-04-29T00:24:31.384373Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9388\nPrecision: 0.9407\nRecall: 0.9388\nF1 Score: 0.9385\nAUC: 0.9968090414951729\nMCC: 0.8804\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Saving model","metadata":{}},{"cell_type":"code","source":"\ntorch.save(model.state_dict(), 'densenet121_disaster.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:24:37.497384Z","iopub.execute_input":"2025-04-29T00:24:37.497665Z","iopub.status.idle":"2025-04-29T00:24:37.593933Z","shell.execute_reply.started":"2025-04-29T00:24:37.497636Z","shell.execute_reply":"2025-04-29T00:24:37.593099Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\n\nprint(os.listdir('/kaggle/working'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T00:24:53.983088Z","iopub.execute_input":"2025-04-29T00:24:53.983801Z","iopub.status.idle":"2025-04-29T00:24:53.988067Z","shell.execute_reply.started":"2025-04-29T00:24:53.983772Z","shell.execute_reply":"2025-04-29T00:24:53.987470Z"}},"outputs":[{"name":"stdout","text":"['densenet121_disaster.pth', '.virtual_documents']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}